





# =========================================================
# STEP 1: SETUP & LOAD DATA
# =========================================================
import pandas as pd
from pathlib import Path

# โหลด dataset ที่คุณสร้างจากขั้น Feature Engineering
DATA_PATH = Path(r"S:\BusinessAnalyticProject\data\processed\walmart_features_weekly.csv")

df = pd.read_csv(DATA_PATH, parse_dates=['Date'])
df.rename(columns={'Target': 'Target_next_week'}, inplace=True)

# กำหนด columns
target_col = 'Target_next_week'
meta_cols = ['Store', 'Date', 'Weekly_Sales', 'Weekly_Sales_Real']
feature_cols = [c for c in df.columns if c not in meta_cols + [target_col]]

# แบ่งชุด train/test ตามเวลา
train_mask = df['Date'] < pd.Timestamp('2012-01-01')
test_mask  = df['Date'] >= pd.Timestamp('2012-01-01')

X_train, y_train = df.loc[train_mask, feature_cols], df.loc[train_mask, target_col]
X_test, y_test   = df.loc[test_mask,  feature_cols], df.loc[test_mask,  target_col]

print(f"Train size: {X_train.shape}, Test size: {X_test.shape}")



# =========================================================
# STEP 2: BASELINE EVALUATION (แก้ version error แล้ว)
# =========================================================
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error

baseline = XGBRegressor(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
baseline.fit(X_train, y_train)
y_pred_base = baseline.predict(X_test)

# ✅ ใช้ mean_squared_error(squared=False) ถ้าใช้ sklearn >= 0.24
# ✅ ถ้า sklearn version เก่ากว่า ให้ใช้ np.sqrt(mean_squared_error(...))
try:
    rmse_base = mean_squared_error(y_test, y_pred_base, squared=False)
except TypeError:
    import numpy as np
    rmse_base = np.sqrt(mean_squared_error(y_test, y_pred_base))

mape_base = mean_absolute_percentage_error(y_test, y_pred_base) * 100

print(f"Baseline RMSE: {rmse_base:,.0f}")
print(f"Baseline MAPE: {mape_base:.2f}%")









# =========================================================
# STEP 3: HYPERPARAMETER TUNING (XGBOOST)
# =========================================================
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from xgboost import XGBRegressor

tscv = TimeSeriesSplit(n_splits=5)
param_grid = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.1, 0.05, 0.01],
    'n_estimators': [300, 500, 800],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

xgb = XGBRegressor(random_state=42, tree_method='hist', eval_metric='rmse')

grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    scoring='neg_root_mean_squared_error',
    cv=tscv,
    verbose=1,
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best CV RMSE:", -grid_search.best_score_)






# STEP 4: TRAIN FINAL MODEL (robust to xgboost version)
from xgboost import XGBRegressor
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error

final_xgb = XGBRegressor(
    **grid_search.best_params_,
    random_state=42,
    tree_method='hist',
    eval_metric='rmse'
)

# 90/10 validation split จาก train
idx = np.arange(len(X_train))
cut = int(len(idx) * 0.9)
X_tr, y_tr = X_train.iloc[:cut], y_train.iloc[:cut]
X_val, y_val = X_train.iloc[cut:], y_train.iloc[cut:]

# ---- Try 1: ใช้ early_stopping_rounds ถ้าเวอร์ชันรองรับ ----
fit_ok = False
try:
    final_xgb.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=100,
        verbose=False
    )
    fit_ok = True
except TypeError:
    pass

# ---- Try 2: ใช้ callbacks.EarlyStopping (ในบางเวอร์ชัน) ----
if not fit_ok:
    try:
        from xgboost.callback import EarlyStopping
        final_xgb.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            callbacks=[EarlyStopping(rounds=100, save_best=True)],
            verbose=False
        )
        fit_ok = True
    except Exception:
        pass

# ---- Fallback: ไม่มี early stopping ก็ fit ปกติ ----
if not fit_ok:
    final_xgb.fit(X_tr, y_tr)

# ---- Evaluate on test ----
y_pred = final_xgb.predict(X_test)

try:
    rmse_tuned = mean_squared_error(y_test, y_pred, squared=False)
except TypeError:
    import numpy as np
    rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred))

mape_tuned = mean_absolute_percentage_error(y_test, y_pred) * 100

print(f"Tuned XGB RMSE: {rmse_tuned:,.0f}")
print(f"Tuned XGB MAPE: {mape_tuned:.2f}%")






# =========================================================
# STEP 5: MODEL COMPARISON (robust metrics)
# =========================================================
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

# helpers: ทำงานได้ทั้ง sklearn เก่า/ใหม่
def rmse_compat(y_true, y_pred):
    try:
        return mean_squared_error(y_true, y_pred, squared=False)  # sklearn >= 0.24
    except TypeError:
        return np.sqrt(mean_squared_error(y_true, y_pred))        # fallback

def mape_compat(y_true, y_pred):
    den = np.maximum(np.abs(np.asarray(y_true)), 1e-9)
    return np.mean(np.abs((np.asarray(y_true) - np.asarray(y_pred)) / den)) * 100

models = {
    "XGBoost": final_xgb,  # โมเดลที่จูนแล้วจาก step 4
    "RandomForest": RandomForestRegressor(n_estimators=800, random_state=42, n_jobs=-1),
    "LightGBM": LGBMRegressor(n_estimators=800, learning_rate=0.05, random_state=42)
}

for name, model in models.items():
    # ถ้ายังไม่ fit (เช่น RF, LGBM) ให้ fit ก่อน
    if hasattr(model, "fit") and not hasattr(model, "best_iteration_"):
        # final_xgb ถูก fit แล้วจาก step 4; RF/LGBM ต้อง fit ที่นี่
        try:
            model.fit(X_train, y_train)
        except TypeError:
            model.fit(X_train.values, y_train.values)  # เผื่อบางเวอร์ชันต้องการ numpy array

    y_pred = model.predict(X_test)
    rmse = rmse_compat(y_test, y_pred)
    mape = mape_compat(y_test, y_pred)
    print(f"{name:12s} | RMSE: {rmse:,.0f} | MAPE: {mape:.2f}%")






# =========================================================
# STEP 6: FEATURE IMPORTANCE & PER-STORE ERROR
# =========================================================
import pandas as pd
import numpy as np

test_df = df.loc[test_mask, ['Store','Date']].copy()
test_df['y_true'] = y_test.values
test_df['y_pred'] = final_xgb.predict(X_test)
test_df['abs_pct_err'] = np.abs((test_df['y_true']-test_df['y_pred']) / np.maximum(np.abs(test_df['y_true']), 1e-9))*100

per_store = (test_df.groupby('Store')['abs_pct_err']
             .mean().sort_values().rename('MAPE_%').to_frame())
print(per_store.head(10))  # top stores (แม่นสุด)
print(per_store.tail(10))  # bottom stores (พลาดสุด)

fi = (pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': final_xgb.feature_importances_
}).sort_values('Importance', ascending=False).head(15))
print(fi)






# =========================================================
# STEP 7: SAVE ARTIFACTS (tables & charts)
# =========================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

OUT_DIR = Path("./reports/artifacts")
OUT_DIR.mkdir(parents=True, exist_ok=True)

# 7.1 Save per-store MAPE
per_store.to_csv(OUT_DIR / "per_store_mape.csv")

# 7.2 Save feature importance top-30
fi30 = (pd.DataFrame({'Feature': X_train.columns,
                      'Importance': final_xgb.feature_importances_})
        .sort_values('Importance', ascending=False).head(30))
fi30.to_csv(OUT_DIR / "feature_importance_top30.csv", index=False)

# 7.3 Plot: Top 15 Feature Importance (matplotlib, ไม่ระบุสี)
plt.figure(figsize=(7,5))
plt.barh(fi30['Feature'].head(15)[::-1], fi30['Importance'].head(15)[::-1])
plt.title("XGBoost Feature Importance (Top 15)")
plt.tight_layout()
plt.savefig(OUT_DIR / "feature_importance_top15.png", dpi=150)
plt.close()

# 7.4 Plot: Per-store MAPE (Top 10 ดีสุด/แย่สุด)
top10  = per_store.head(10).reset_index()
bot10  = per_store.tail(10).reset_index()

plt.figure(figsize=(6,4))
plt.bar(top10['Store'].astype(str), top10['MAPE_%'])
plt.title("Top 10 Stores (Lowest MAPE)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(OUT_DIR / "per_store_mape_top10.png", dpi=150)
plt.close()

plt.figure(figsize=(6,4))
plt.bar(bot10['Store'].astype(str), bot10['MAPE_%'])
plt.title("Bottom 10 Stores (Highest MAPE)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig(OUT_DIR / "per_store_mape_bottom10.png", dpi=150)
plt.close()






# =========================================================
# STEP 8: BIAS CHECKS (Holiday/Season)
# =========================================================
# ต้องมีคอลัมน์พวก Peak_Season_Flag หรือ Holiday_Flag ใน X_test
cols_flag = [c for c in ['Peak_Season_Flag','Holiday_Flag','Month'] if c in X_test.columns]
print("Flag cols found:", cols_flag)

# เพิ่มคอลัมน์ error ลง test_df
test_df['err'] = test_df['y_true'] - test_df['y_pred']
test_df['abs_err'] = np.abs(test_df['err'])

# 8.1 MAPE by Holiday/Peak Season
for c in ['Peak_Season_Flag','Holiday_Flag']:
    if c in X_test.columns:
        tmp = X_test[[c]].copy()
        tmp['mape'] = test_df['abs_pct_err'].values
        print(f"\nMAPE by {c}")
        print(tmp.groupby(c)['mape'].mean())

# 8.2 MAPE by Month (ดูฤดูกาล)
if 'Month' in X_test.columns:
    tmp = X_test[['Month']].copy()
    tmp['mape'] = test_df['abs_pct_err'].values
    print("\nMAPE by Month")
    print(tmp.groupby('Month')['mape'].mean().round(2))






# =========================================================
# STEP 9: SUMMARY TEXT (for README/report)
# =========================================================
from sklearn.metrics import mean_squared_error
import numpy as np

# baseline vs tuned (ถ้ามีตัวแปร rmse_base/mape_base จาก step 2)
def rmse_compat(y_true, y_pred):
    try:
        return mean_squared_error(y_true, y_pred, squared=False)
    except TypeError:
        return np.sqrt(mean_squared_error(y_true, y_pred))

summary = {
    "Baseline_RMSE": f"{rmse_base:,.0f}" if 'rmse_base' in globals() else "NA",
    "Baseline_MAPE": f"{mape_base:.2f}%" if 'mape_base' in globals() else "NA",
    "Tuned_XGB_RMSE": f"{rmse_compat(test_df['y_true'], test_df['y_pred']):,.0f}",
    "Tuned_XGB_MAPE": f"{test_df['abs_pct_err'].mean():.2f}%",
    "Best_Params": grid_search.best_params_
}
print("=== MODEL IMPROVEMENT SUMMARY ===")
for k,v in summary.items():
    print(f"{k}: {v}")

# Save summary json
import json
with open(OUT_DIR / "model_improvement_summary.json", "w") as f:
    json.dump(summary, f, indent=2, default=str)




