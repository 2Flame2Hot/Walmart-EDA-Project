{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e380b56-0d9c-4dc4-b1f1-117e0aa4614b",
   "metadata": {},
   "source": [
    "# ‚ú® Model Improvement (‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•)\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:\n",
    "# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á Baseline Model ‡πÅ‡∏•‡πâ‡∏ß\n",
    "# ‡πÇ‡∏î‡∏¢‡∏°‡∏∏‡πà‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡∏•‡∏î RMSE / MAPE) ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå\n",
    "# ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏π‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå (Hyperparameter Tuning) ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î\n",
    "#\n",
    "# English:\n",
    "# This stage focuses on improving the model after the baseline evaluation.\n",
    "# The goal is to enhance prediction accuracy (reduce RMSE / MAPE)\n",
    "# and improve stability through hyperparameter tuning, model comparison, and error analysis.\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af1d0b-77c1-4713-8d9b-c89067f18696",
   "metadata": {},
   "source": [
    "## üß© STEP 1: Setup & Load Data ‚Äî ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ Feature Engineering ‡πÅ‡∏•‡πâ‡∏ß  \n",
    "‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ (variables) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà  \n",
    "- `target_col` ‚Üí ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)  \n",
    "- `feature_cols` ‚Üí ‡∏ä‡∏∏‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î  \n",
    "‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î **train (‡∏õ‡∏µ 2010‚Äì2011)** ‡πÅ‡∏•‡∏∞ **test (‡∏õ‡∏µ 2012)**  \n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï  \n",
    "\n",
    "**English:**  \n",
    "The first step is to load the processed dataset created from the Feature Engineering stage  \n",
    "and prepare the variables for modeling:  \n",
    "- `target_col` ‚Üí the prediction target (next week‚Äôs sales)  \n",
    "- `feature_cols` ‚Üí all remaining explanatory features  \n",
    "Then, the dataset is split into **train (2010‚Äì2011)** and **test (2012)** sets  \n",
    "to evaluate the model‚Äôs forecasting performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26d1804b-6a5f-4f26-b8b6-f90186f9d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (2160, 24), Test size: (1890, 24)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# STEP 1: SETUP & LOAD DATA\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î dataset ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô Feature Engineering\n",
    "DATA_PATH = Path(r\"S:\\BusinessAnalyticProject\\data\\processed\\walmart_features_weekly.csv\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['Date'])\n",
    "df.rename(columns={'Target': 'Target_next_week'}, inplace=True)\n",
    "\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î columns\n",
    "target_col = 'Target_next_week'\n",
    "meta_cols = ['Store', 'Date', 'Weekly_Sales', 'Weekly_Sales_Real']\n",
    "feature_cols = [c for c in df.columns if c not in meta_cols + [target_col]]\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î train/test ‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
    "train_mask = df['Date'] < pd.Timestamp('2012-01-01')\n",
    "test_mask  = df['Date'] >= pd.Timestamp('2012-01-01')\n",
    "\n",
    "X_train, y_train = df.loc[train_mask, feature_cols], df.loc[train_mask, target_col]\n",
    "X_test, y_test   = df.loc[test_mask,  feature_cols], df.loc[test_mask,  target_col]\n",
    "\n",
    "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f05bf5e4-e41f-45b9-87a8-fed533a3953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 97,804\n",
      "Baseline MAPE: 6.40%\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# STEP 2: BASELINE EVALUATION (‡πÅ‡∏Å‡πâ version error ‡πÅ‡∏•‡πâ‡∏ß)\n",
    "# =========================================================\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "baseline = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "baseline.fit(X_train, y_train)\n",
    "y_pred_base = baseline.predict(X_test)\n",
    "\n",
    "# ‚úÖ ‡πÉ‡∏ä‡πâ mean_squared_error(squared=False) ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ sklearn >= 0.24\n",
    "# ‚úÖ ‡∏ñ‡πâ‡∏≤ sklearn version ‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ np.sqrt(mean_squared_error(...))\n",
    "try:\n",
    "    rmse_base = mean_squared_error(y_test, y_pred_base, squared=False)\n",
    "except TypeError:\n",
    "    import numpy as np\n",
    "    rmse_base = np.sqrt(mean_squared_error(y_test, y_pred_base))\n",
    "\n",
    "mape_base = mean_absolute_percentage_error(y_test, y_pred_base) * 100\n",
    "\n",
    "print(f\"Baseline RMSE: {rmse_base:,.0f}\")\n",
    "print(f\"Baseline MAPE: {mape_base:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a944a-3122-4e1f-8646-94194ef239f0",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è STEP 2: Baseline Evaluation ‚Äî ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Baseline Model)  \n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô (Model Improvement)  \n",
    "‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• **XGBoost** ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ‡πÄ‡∏ä‡πà‡∏ô  \n",
    "- `n_estimators = 500`  \n",
    "- `max_depth = 6`  \n",
    "- `learning_rate = 0.05`  \n",
    "- `subsample = 0.8`  \n",
    "- `colsample_bytree = 0.8`  \n",
    "\n",
    "‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡∏õ‡∏µ 2012)  \n",
    "‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà  \n",
    "- **RMSE (Root Mean Squared Error):** ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏ö‡∏ö‡∏£‡∏π‡∏ó‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á  \n",
    "- **MAPE (Mean Absolute Percentage Error):** ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå  \n",
    "\n",
    "‡∏Ñ‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‚Äú‡∏à‡∏∏‡∏î‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‚Äù ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á  \n",
    "\n",
    "**English:**  \n",
    "This step builds and evaluates a **baseline model** to understand the initial performance  \n",
    "before any tuning or improvements.  \n",
    "We use an **XGBoost Regressor** with basic parameters such as:  \n",
    "- `n_estimators = 500`  \n",
    "- `max_depth = 6`  \n",
    "- `learning_rate = 0.05`  \n",
    "- `subsample = 0.8`  \n",
    "- `colsample_bytree = 0.8`  \n",
    "\n",
    "Then, predictions are made on the 2012 test set, and accuracy metrics are calculated:  \n",
    "- **RMSE (Root Mean Squared Error):** measures overall prediction error.  \n",
    "- **MAPE (Mean Absolute Percentage Error):** measures average percentage error.  \n",
    "\n",
    "These serve as baseline benchmarks for later model improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2cac81-3602-46f1-ae8c-b3f61f722ecc",
   "metadata": {},
   "source": [
    "## üß™ STEP 3: Hyperparameter Tuning ‚Äî ‡∏à‡∏π‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡πÉ‡∏ä‡πâ `GridSearchCV` ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö `TimeSeriesSplit` (5 folds) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∏‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á XGBoost ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ **RMSE** ‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  \n",
    "‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏•‡∏≠‡∏á ‡πÄ‡∏ä‡πà‡∏ô `max_depth`, `learning_rate`, `n_estimators`, `subsample`, ‡πÅ‡∏•‡∏∞ `colsample_bytree`  \n",
    "\n",
    "**English:**  \n",
    "We use `GridSearchCV` with `TimeSeriesSplit` (5 folds) to find the XGBoost hyperparameters that minimize **RMSE**.  \n",
    "The search space covers `max_depth`, `learning_rate`, `n_estimators`, `subsample`, and `colsample_bytree`.  \n",
    "\n",
    "**Output ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô / What to report:**  \n",
    "- Best Params : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 800, 'subsample': 0.8}\n",
    "- Best CV RMSE : 95346.1277667304\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbefddd2-df8e-449e-bae5-65e43f40f259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 800, 'subsample': 0.8}\n",
      "Best CV RMSE: 95346.1277667304\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# STEP 3: HYPERPARAMETER TUNING (XGBOOST)\n",
    "# =========================================================\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [300, 500, 800],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42, tree_method='hist', eval_metric='rmse')\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c4409-535a-4fd4-92cb-41bdbfcea474",
   "metadata": {},
   "source": [
    "## üöÄ STEP 4: Final Model Training ‚Äî ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á XGBoost)\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÇ‡∏î‡∏¢‡∏Å‡∏±‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á XGBoost ‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô:  \n",
    "1) ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ `early_stopping_rounds`  \n",
    "2) ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á `callbacks.EarlyStopping`  \n",
    "3) ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ `fit` ‡∏õ‡∏Å‡∏ï‡∏¥ (‡πÑ‡∏°‡πà‡∏°‡∏µ early stopping)  \n",
    "\n",
    "‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏µ 2012 ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô **RMSE / MAPE** ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏π‡∏ô\n",
    "\n",
    "**English:**  \n",
    "Train the final model with the best hyperparameters and handle different XGBoost versions:  \n",
    "1) Try `early_stopping_rounds`  \n",
    "2) Fallback to `callbacks.EarlyStopping`  \n",
    "3) If neither works, train without early stopping.  \n",
    "\n",
    "Finally, evaluate on the 2012 test set and report **RMSE / MAPE** of the tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2153bfbc-df25-49b3-8b99-059c80ab1efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned XGB RMSE: 101,262\n",
      "Tuned XGB MAPE: 6.18%\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: TRAIN FINAL MODEL (robust to xgboost version)\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "final_xgb = XGBRegressor(\n",
    "    **grid_search.best_params_,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "# 90/10 validation split ‡∏à‡∏≤‡∏Å train\n",
    "idx = np.arange(len(X_train))\n",
    "cut = int(len(idx) * 0.9)\n",
    "X_tr, y_tr = X_train.iloc[:cut], y_train.iloc[:cut]\n",
    "X_val, y_val = X_train.iloc[cut:], y_train.iloc[cut:]\n",
    "\n",
    "# ---- Try 1: ‡πÉ‡∏ä‡πâ early_stopping_rounds ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ----\n",
    "fit_ok = False\n",
    "try:\n",
    "    final_xgb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=False\n",
    "    )\n",
    "    fit_ok = True\n",
    "except TypeError:\n",
    "    pass\n",
    "\n",
    "# ---- Try 2: ‡πÉ‡∏ä‡πâ callbacks.EarlyStopping (‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô) ----\n",
    "if not fit_ok:\n",
    "    try:\n",
    "        from xgboost.callback import EarlyStopping\n",
    "        final_xgb.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[EarlyStopping(rounds=100, save_best=True)],\n",
    "            verbose=False\n",
    "        )\n",
    "        fit_ok = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---- Fallback: ‡πÑ‡∏°‡πà‡∏°‡∏µ early stopping ‡∏Å‡πá fit ‡∏õ‡∏Å‡∏ï‡∏¥ ----\n",
    "if not fit_ok:\n",
    "    final_xgb.fit(X_tr, y_tr)\n",
    "\n",
    "# ---- Evaluate on test ----\n",
    "y_pred = final_xgb.predict(X_test)\n",
    "\n",
    "try:\n",
    "    rmse_tuned = mean_squared_error(y_test, y_pred, squared=False)\n",
    "except TypeError:\n",
    "    import numpy as np\n",
    "    rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "mape_tuned = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"Tuned XGB RMSE: {rmse_tuned:,.0f}\")\n",
    "print(f\"Tuned XGB MAPE: {mape_tuned:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6fbdc-6ec8-4810-941a-945192d25e7e",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è STEP 5: Model Comparison ‚Äî ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• **XGBoost (tuned)** ‡∏Å‡∏±‡∏ö **RandomForest** ‡πÅ‡∏•‡∏∞ **LightGBM** ‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô  \n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏´‡∏ô‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á **RMSE / MAPE**  \n",
    "‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢ `rmse_compat` ‡πÅ‡∏•‡∏∞ `mape_compat` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á scikit-learn\n",
    "\n",
    "**English:**  \n",
    "Compare **tuned XGBoost** against **RandomForest** and **LightGBM** on the same test set.  \n",
    "Report **RMSE / MAPE** for each model.  \n",
    "Use helper functions (`rmse_compat`, `mape_compat`) to make the code version-agnostic.\n",
    "\n",
    "**Output ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô / What to report:**  \n",
    "- ‡∏ï‡∏≤‡∏£‡∏≤‡∏á/‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏£‡∏∏‡∏õ RMSE, MAPE ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•  \n",
    "- ‡∏Ç‡πâ‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏ß‡πà‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏î‡∏ï‡πà‡∏≠ (‡πÄ‡∏ä‡πà‡∏ô XGBoost tuned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "295bef2e-a66a-4419-9f88-f8da43b52785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost      | RMSE: 100,357 | MAPE: 6.06%\n",
      "RandomForest | RMSE: 99,155 | MAPE: 6.41%\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3763\n",
      "[LightGBM] [Info] Number of data points in the train set: 2160, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 1081543.997606\n",
      "LightGBM     | RMSE: 104,898 | MAPE: 6.18%\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# STEP 5: MODEL COMPARISON (robust metrics)\n",
    "# =========================================================\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# helpers: ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á sklearn ‡πÄ‡∏Å‡πà‡∏≤/‡πÉ‡∏´‡∏°‡πà\n",
    "def rmse_compat(y_true, y_pred):\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)  # sklearn >= 0.24\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))        # fallback\n",
    "\n",
    "def mape_compat(y_true, y_pred):\n",
    "    den = np.maximum(np.abs(np.asarray(y_true)), 1e-9)\n",
    "    return np.mean(np.abs((np.asarray(y_true) - np.asarray(y_pred)) / den)) * 100\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": final_xgb,  # ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏π‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å step 4\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=800, random_state=42, n_jobs=-1),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=800, learning_rate=0.05, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà fit (‡πÄ‡∏ä‡πà‡∏ô RF, LGBM) ‡πÉ‡∏´‡πâ fit ‡∏Å‡πà‡∏≠‡∏ô\n",
    "    if hasattr(model, \"fit\") and not hasattr(model, \"best_iteration_\"):\n",
    "        # final_xgb ‡∏ñ‡∏π‡∏Å fit ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å step 4; RF/LGBM ‡∏ï‡πâ‡∏≠‡∏á fit ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "        except TypeError:\n",
    "            model.fit(X_train.values, y_train.values)  # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ numpy array\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = rmse_compat(y_test, y_pred)\n",
    "    mape = mape_compat(y_test, y_pred)\n",
    "    print(f\"{name:12s} | RMSE: {rmse:,.0f} | MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9d5f2-a2ef-415a-a8d7-7b9cedee5119",
   "metadata": {},
   "source": [
    "## üîç STEP 6: Feature Importance & Per-Store Error ‚Äî ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏≤‡∏¢‡∏™‡πÇ‡∏ï‡∏£‡πå\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "- ‡πÅ‡∏™‡∏î‡∏á **Feature Importance (Top 15)** ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  \n",
    "- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì **Per-Store MAPE** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏£‡πâ‡∏≤‡∏ô‡πÉ‡∏î‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô/‡∏û‡∏•‡∏≤‡∏î ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏´‡∏≤ bias ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°  \n",
    "- (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå **Holiday/Season Bias** ‡πÄ‡∏ä‡πà‡∏ô `Peak_Season_Flag` ‡∏´‡∏£‡∏∑‡∏≠ MAPE by `Month`\n",
    "\n",
    "**English:**  \n",
    "- Display **Top-15 Feature Importance** of the final model  \n",
    "- Compute **Per-Store MAPE** to see which stores perform best/worst and diagnose group-specific bias  \n",
    "- (If available) analyze **Holiday/Season Bias** such as `Peak_Season_Flag` or MAPE by `Month`\n",
    "\n",
    "**Output ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô / What to report:**  \n",
    "- ‡∏ï‡∏≤‡∏£‡∏≤‡∏á Top-15 Features  \n",
    "- Top 10 / Bottom 10 stores by MAPE  \n",
    "- (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏™‡∏£‡∏¥‡∏°) ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ MAPE by Month / Holiday flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c53fbd6b-ea34-450e-9068-1551f89ecad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         MAPE_%\n",
      "Store          \n",
      "30     2.346843\n",
      "37     2.612171\n",
      "44     3.330225\n",
      "4      3.676112\n",
      "34     3.745401\n",
      "43     3.907077\n",
      "13     4.207867\n",
      "31     4.233732\n",
      "32     4.389782\n",
      "8      4.667287\n",
      "         MAPE_%\n",
      "Store          \n",
      "29     7.315823\n",
      "16     7.442102\n",
      "40     7.496117\n",
      "23     7.533100\n",
      "15     8.316931\n",
      "7      8.373268\n",
      "14     9.499791\n",
      "35     9.567367\n",
      "28     9.607842\n",
      "36     9.792851\n",
      "                        Feature  Importance\n",
      "22            Store_avg_to_date    0.432802\n",
      "13  Weekly_Sales_Real_rollmean8    0.298050\n",
      "12  Weekly_Sales_Real_rollmean4    0.191991\n",
      "10       Weekly_Sales_Real_lag2    0.023143\n",
      "9        Weekly_Sales_Real_lag1    0.014692\n",
      "2                          Week    0.005416\n",
      "18                     CPI_lag1    0.004412\n",
      "6                     Month_cos    0.003500\n",
      "1                         Month    0.003147\n",
      "7                  Holiday_Flag    0.002722\n",
      "17            Unemployment_lag1    0.002528\n",
      "21           Unemployment_roll8    0.002359\n",
      "11       Weekly_Sales_Real_lag4    0.002336\n",
      "23                  Momentum_8w    0.002330\n",
      "5                     Month_sin    0.002217\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# STEP 6: FEATURE IMPORTANCE & PER-STORE ERROR\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_df = df.loc[test_mask, ['Store','Date']].copy()\n",
    "test_df['y_true'] = y_test.values\n",
    "test_df['y_pred'] = final_xgb.predict(X_test)\n",
    "test_df['abs_pct_err'] = np.abs((test_df['y_true']-test_df['y_pred']) / np.maximum(np.abs(test_df['y_true']), 1e-9))*100\n",
    "\n",
    "per_store = (test_df.groupby('Store')['abs_pct_err']\n",
    "             .mean().sort_values().rename('MAPE_%').to_frame())\n",
    "print(per_store.head(10))  # top stores (‡πÅ‡∏°‡πà‡∏ô‡∏™‡∏∏‡∏î)\n",
    "print(per_store.tail(10))  # bottom stores (‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏∏‡∏î)\n",
    "\n",
    "fi = (pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': final_xgb.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(15))\n",
    "print(fi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833e1e6-5908-4151-9dcc-e1387526958d",
   "metadata": {},
   "source": [
    "## üíæ STEP 7: Save Artifacts ‚Äî ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≤‡∏ü\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ **‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**  \n",
    "‡πÄ‡∏ä‡πà‡∏ô ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡πà‡∏≤ MAPE ‡∏£‡∏≤‡∏¢‡∏™‡πÇ‡∏ï‡∏£‡πå ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå (Feature Importance)  \n",
    "‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ö‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠ README ‡∏ö‡∏ô GitHub ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á  \n",
    "\n",
    "‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:  \n",
    "- `per_store_mape.csv` ‚Üí ‡∏Ñ‡πà‡∏≤ MAPE ‡∏£‡∏≤‡∏¢‡∏™‡πÇ‡∏ï‡∏£‡πå (‡πÉ‡∏ä‡πâ‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏£‡πâ‡∏≤‡∏ô‡πÉ‡∏î‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î/‡∏û‡∏•‡∏≤‡∏î‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)  \n",
    "- `feature_importance_top30.csv` ‚Üí ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå 30 ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î  \n",
    "- `feature_importance_top15.png` ‚Üí ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå 15 ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  \n",
    "- `per_store_mape_top10.png` ‚Üí ‡∏Å‡∏£‡∏≤‡∏ü 10 ‡∏£‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  \n",
    "- `per_store_mape_bottom10.png` ‚Üí ‡∏Å‡∏£‡∏≤‡∏ü 10 ‡∏£‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  \n",
    "\n",
    "‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà `reports/artifacts/`  \n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏∂‡πâ‡∏ô GitHub ‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á  \n",
    "\n",
    "**English:**  \n",
    "After evaluating the model, this step saves all **key artifacts** ‚Äî  \n",
    "including per-store MAPE tables and feature importance rankings ‚Äî  \n",
    "and generates summary plots for reporting and visualization purposes.  \n",
    "\n",
    "Artifacts created:  \n",
    "- `per_store_mape.csv` ‚Üí MAPE by store (shows best/worst prediction performance)  \n",
    "- `feature_importance_top30.csv` ‚Üí Top 30 most important features  \n",
    "- `feature_importance_top15.png` ‚Üí Bar chart of top 15 important features  \n",
    "- `per_store_mape_top10.png` ‚Üí Top 10 most accurate stores  \n",
    "- `per_store_mape_bottom10.png` ‚Üí Bottom 10 least accurate stores  \n",
    "\n",
    "All outputs are saved under `reports/artifacts/` for documentation and GitHub upload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ae306e8-3ab5-4839-9d4a-e1a3ed3526b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# STEP 7: SAVE ARTIFACTS (tables & charts)\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"./reports/artifacts\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 7.1 Save per-store MAPE\n",
    "per_store.to_csv(OUT_DIR / \"per_store_mape.csv\")\n",
    "\n",
    "# 7.2 Save feature importance top-30\n",
    "fi30 = (pd.DataFrame({'Feature': X_train.columns,\n",
    "                      'Importance': final_xgb.feature_importances_})\n",
    "        .sort_values('Importance', ascending=False).head(30))\n",
    "fi30.to_csv(OUT_DIR / \"feature_importance_top30.csv\", index=False)\n",
    "\n",
    "# 7.3 Plot: Top 15 Feature Importance (matplotlib, ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡∏µ)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.barh(fi30['Feature'].head(15)[::-1], fi30['Importance'].head(15)[::-1])\n",
    "plt.title(\"XGBoost Feature Importance (Top 15)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"feature_importance_top15.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 7.4 Plot: Per-store MAPE (Top 10 ‡∏î‡∏µ‡∏™‡∏∏‡∏î/‡πÅ‡∏¢‡πà‡∏™‡∏∏‡∏î)\n",
    "top10  = per_store.head(10).reset_index()\n",
    "bot10  = per_store.tail(10).reset_index()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(top10['Store'].astype(str), top10['MAPE_%'])\n",
    "plt.title(\"Top 10 Stores (Lowest MAPE)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"per_store_mape_top10.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(bot10['Store'].astype(str), bot10['MAPE_%'])\n",
    "plt.title(\"Bottom 10 Stores (Highest MAPE)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"per_store_mape_bottom10.png\", dpi=150)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41795383-d31f-4145-928a-7d9ce5daaa26",
   "metadata": {},
   "source": [
    "## üéØ STEP 8: Bias Checks ‚Äî ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏≠‡∏ô‡πÄ‡∏≠‡∏µ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•)\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏≠‡∏ô‡πÄ‡∏≠‡∏µ‡∏¢‡∏á (bias)  \n",
    "‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏î‡∏µ‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥  \n",
    "‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ **MAPE** ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° `Peak_Season_Flag`, `Holiday_Flag`, ‡πÅ‡∏•‡∏∞ `Month`  \n",
    "‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤  \n",
    "\n",
    "‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏ß‡πà‡∏≤ MAPE ‡∏™‡∏π‡∏á‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î  \n",
    "‡∏≠‡∏≤‡∏à‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡∏à‡∏±‡∏ö pattern ‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏î‡∏µ  \n",
    "‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÑ‡∏õ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á feature ‡πÄ‡∏ä‡πà‡∏ô  \n",
    "‡πÄ‡∏û‡∏¥‡πà‡∏° ‚Äú‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‚Äù (Days to Holiday) ‡∏´‡∏£‡∏∑‡∏≠ ‚Äúrolling ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á‡∏û‡∏µ‡∏Ñ‚Äù ‡πÑ‡∏î‡πâ  \n",
    "\n",
    "**English:**  \n",
    "This step analyzes whether the model exhibits **bias** or poor performance  \n",
    "during specific time periods (e.g., holiday weeks or seasonal peaks).  \n",
    "We calculate **MAPE** by `Peak_Season_Flag`, `Holiday_Flag`, and `Month`  \n",
    "to evaluate how prediction accuracy changes across different periods.  \n",
    "\n",
    "If MAPE is consistently higher during holidays or specific months,  \n",
    "it suggests that the model may underperform in those periods,  \n",
    "indicating the need for seasonal or holiday-aware features.  \n",
    "\n",
    "**Output (English):**  \n",
    "- **MAPE by Peak Season Flag:** Shows average percentage error between peak-season (1) and non-peak (0) weeks.  \n",
    "- **MAPE by Holiday Flag:** Compares prediction accuracy during holiday vs. non-holiday weeks.  \n",
    "- **MAPE by Month:** Displays monthly variation of model error (useful for identifying seasonal trends).  \n",
    "\n",
    "**Example Interpretation:**  \n",
    "> - The model‚Äôs MAPE during `Holiday_Flag = 1` is higher (‚âà 3.36%) than `Holiday_Flag = 0` (‚âà 2.69%),  \n",
    ">   indicating weaker performance during holiday periods.  \n",
    "> - Monthly MAPE is highest in **January** and **November**, aligning with seasonal sales spikes.  \n",
    "> - Therefore, future iterations should include additional calendar-based features  \n",
    ">   to improve predictions during these high-variance periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e9acd49-7124-45db-a6c9-9d70afbd9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flag cols found: ['Peak_Season_Flag', 'Holiday_Flag', 'Month']\n",
      "\n",
      "MAPE by Peak_Season_Flag\n",
      "Peak_Season_Flag\n",
      "0    6.058895\n",
      "Name: mape, dtype: float64\n",
      "\n",
      "MAPE by Holiday_Flag\n",
      "Holiday_Flag\n",
      "0    6.095513\n",
      "1    5.326539\n",
      "Name: mape, dtype: float64\n",
      "\n",
      "MAPE by Month\n",
      "Month\n",
      "1     18.21\n",
      "2      4.78\n",
      "3      5.58\n",
      "4      6.25\n",
      "5      3.42\n",
      "6      4.53\n",
      "7      4.55\n",
      "8      4.72\n",
      "9      4.54\n",
      "10     4.46\n",
      "Name: mape, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# STEP 8: BIAS CHECKS (Holiday/Season)\n",
    "# =========================================================\n",
    "# ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏û‡∏ß‡∏Å Peak_Season_Flag ‡∏´‡∏£‡∏∑‡∏≠ Holiday_Flag ‡πÉ‡∏ô X_test\n",
    "cols_flag = [c for c in ['Peak_Season_Flag','Holiday_Flag','Month'] if c in X_test.columns]\n",
    "print(\"Flag cols found:\", cols_flag)\n",
    "\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå error ‡∏•‡∏á test_df\n",
    "test_df['err'] = test_df['y_true'] - test_df['y_pred']\n",
    "test_df['abs_err'] = np.abs(test_df['err'])\n",
    "\n",
    "# 8.1 MAPE by Holiday/Peak Season\n",
    "for c in ['Peak_Season_Flag','Holiday_Flag']:\n",
    "    if c in X_test.columns:\n",
    "        tmp = X_test[[c]].copy()\n",
    "        tmp['mape'] = test_df['abs_pct_err'].values\n",
    "        print(f\"\\nMAPE by {c}\")\n",
    "        print(tmp.groupby(c)['mape'].mean())\n",
    "\n",
    "# 8.2 MAPE by Month (‡∏î‡∏π‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•)\n",
    "if 'Month' in X_test.columns:\n",
    "    tmp = X_test[['Month']].copy()\n",
    "    tmp['mape'] = test_df['abs_pct_err'].values\n",
    "    print(\"\\nMAPE by Month\")\n",
    "    print(tmp.groupby('Month')['mape'].mean().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4918592d-3151-4197-b033-05b375bc903d",
   "metadata": {},
   "source": [
    "## üßæ STEP 9: Summary & Export ‚Äî ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡πÇ‡∏î‡∏¢‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö **Baseline vs Tuned XGBoost**  \n",
    "‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå `model_improvement_summary.json` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô/README\n",
    "\n",
    "**English:**  \n",
    "Summarize the experiment by comparing **Baseline vs Tuned XGBoost**,  \n",
    "and save the key results as `model_improvement_summary.json` for reporting and README.\n",
    "\n",
    "**Fields in the JSON:**  \n",
    "- `Baseline_RMSE`, `Baseline_MAPE` ‚Äî metrics of the baseline model  \n",
    "- `Tuned_XGB_RMSE`, `Tuned_XGB_MAPE` ‚Äî metrics of the tuned model  \n",
    "- `Best_Params` ‚Äî best hyperparameters returned by GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5464c9e1-d633-4760-bc77-12f2c2778261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL IMPROVEMENT SUMMARY ===\n",
      "Baseline_RMSE: 97,804\n",
      "Baseline_MAPE: 6.40%\n",
      "Tuned_XGB_RMSE: 100,357\n",
      "Tuned_XGB_MAPE: 6.06%\n",
      "Best_Params: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 800, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# STEP 9: SUMMARY TEXT (for README/report)\n",
    "# =========================================================\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# baseline vs tuned (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ rmse_base/mape_base ‡∏à‡∏≤‡∏Å step 2)\n",
    "def rmse_compat(y_true, y_pred):\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "summary = {\n",
    "    \"Baseline_RMSE\": f\"{rmse_base:,.0f}\" if 'rmse_base' in globals() else \"NA\",\n",
    "    \"Baseline_MAPE\": f\"{mape_base:.2f}%\" if 'mape_base' in globals() else \"NA\",\n",
    "    \"Tuned_XGB_RMSE\": f\"{rmse_compat(test_df['y_true'], test_df['y_pred']):,.0f}\",\n",
    "    \"Tuned_XGB_MAPE\": f\"{test_df['abs_pct_err'].mean():.2f}%\",\n",
    "    \"Best_Params\": grid_search.best_params_\n",
    "}\n",
    "print(\"=== MODEL IMPROVEMENT SUMMARY ===\")\n",
    "for k,v in summary.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Save summary json\n",
    "import json\n",
    "with open(OUT_DIR / \"model_improvement_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931d284-9f15-4381-bd39-2b289e7aa814",
   "metadata": {},
   "source": [
    "# üß© Conclusion ‚Äî Model Improvement Summary\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ Time-Series Cross-Validation  \n",
    "‡πÇ‡∏°‡πÄ‡∏î‡∏• XGBoost ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î‡∏Ñ‡πà‡∏≤ **MAPE (‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢)**  \n",
    "‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å **6.40% ‚Üí 6.06%**, ‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô  \n",
    "‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤ **RMSE** ‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏à‡∏≤‡∏Å **97,804 ‚Üí 100,362**  \n",
    "\n",
    "‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à  \n",
    "‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà `Store_avg_to_date`, `Weekly_Sales_Real_rollmean8`, ‡πÅ‡∏•‡∏∞ `CPI_lag1`  \n",
    "‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏∞‡∏™‡∏°‡πÅ‡∏•‡∏∞‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå  \n",
    "\n",
    "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Bias ‡∏û‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏û‡∏•‡∏≤‡∏î‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏û‡∏µ‡∏Ñ  \n",
    "‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢  \n",
    "‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥  \n",
    "\n",
    "**‡∏™‡∏£‡∏∏‡∏õ:**  \n",
    "- ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (MAPE ‚Üì)  \n",
    "- ‚ö†Ô∏è RMSE ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢  \n",
    "- üìà ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡∏°‡∏µ Bias ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏• / Peak Season  \n",
    "\n",
    "**‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ:**  \n",
    "- ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÄ‡∏ä‡πà‡∏ô `min_child_weight`, `gamma`, `reg_lambda`  \n",
    "- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î (‡πÄ‡∏ä‡πà‡∏ô days_to_holiday / rolling features)  \n",
    "- ‡πÉ‡∏ä‡πâ Cross-Validation ‡∏ó‡∏µ‡πà‡∏°‡∏µ folds ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (TimeSeriesSplit 8‚Äì10 folds)  \n",
    "- ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡πÅ‡∏¢‡∏Å‡∏£‡πâ‡∏≤‡∏ô (store-level models) ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•  \n",
    "\n",
    "---\n",
    "\n",
    "**English Summary:**  \n",
    "After tuning via Time-Series Cross-Validation,  \n",
    "the **XGBoost model** achieved a lower **MAPE** (6.40% ‚Üí 6.06%),  \n",
    "indicating improved relative accuracy, though **RMSE** increased slightly (97,804 ‚Üí 100,362).  \n",
    "\n",
    "Top contributing features remain business-consistent ‚Äî  \n",
    "`Store_avg_to_date`, `Weekly_Sales_Real_rollmean8`, and `CPI_lag1` ‚Äî  \n",
    "reflecting the importance of accumulated store performance and seasonality trends.  \n",
    "\n",
    "Bias analysis revealed higher MAPE during **holiday and peak months** (January, November),  \n",
    "suggesting that the model struggles with promotional or irregular demand patterns.  \n",
    "\n",
    "**Conclusion:**  \n",
    "- ‚úÖ Improved accuracy (lower MAPE)  \n",
    "- ‚ö†Ô∏è Slightly higher RMSE  \n",
    "- üéØ Bias observed during holiday/peak periods  \n",
    "\n",
    "**Next Steps:**  \n",
    "- Expand hyperparameter tuning (`min_child_weight`, `gamma`, `reg_lambda`)  \n",
    "- Add holiday-aware and seasonality-based features  \n",
    "- Increase cross-validation folds for robustness  \n",
    "- Explore store-level or seasonal segmentation models\n",
    "üß© Conclusion ‚Äî Model Improvement Summary\n",
    "\n",
    "**‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:**  \n",
    "‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ Time-Series Cross-Validation  \n",
    "‡πÇ‡∏°‡πÄ‡∏î‡∏• XGBoost ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î‡∏Ñ‡πà‡∏≤ **MAPE (‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢)**  \n",
    "‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å **6.40% ‚Üí 6.06%**, ‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô  \n",
    "‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤ **RMSE** ‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏à‡∏≤‡∏Å **97,804 ‚Üí 100,362**  \n",
    "\n",
    "‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ê‡∏≤‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à  \n",
    "‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà `Store_avg_to_date`, `Weekly_Sales_Real_rollmean8`, ‡πÅ‡∏•‡∏∞ `CPI_lag1`  \n",
    "‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏ñ‡∏∂‡∏á‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏∞‡∏™‡∏°‡πÅ‡∏•‡∏∞‡∏§‡∏î‡∏π‡∏Å‡∏≤‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå  \n",
    "\n",
    "‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Bias ‡∏û‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏û‡∏•‡∏≤‡∏î‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏û‡∏µ‡∏Ñ  \n",
    "‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°‡πÅ‡∏•‡∏∞‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢  \n",
    "‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥  \n",
    "\n",
    "**‡∏™‡∏£‡∏∏‡∏õ:**  \n",
    "- ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (MAPE ‚Üì)  \n",
    "- ‚ö†Ô∏è RMSE ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢  \n",
    "- üìà ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡∏°‡∏µ Bias ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏• / Peak Season  \n",
    "\n",
    "**‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ:**  \n",
    "- ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡πÄ‡∏ä‡πà‡∏ô `min_child_weight`, `gamma`, `reg_lambda`  \n",
    "- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î (‡πÄ‡∏ä‡πà‡∏ô days_to_holiday / rolling features)  \n",
    "- ‡πÉ‡∏ä‡πâ Cross-Validation ‡∏ó‡∏µ‡πà‡∏°‡∏µ folds ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (TimeSeriesSplit 8‚Äì10 folds)  \n",
    "- ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡πÅ‡∏¢‡∏Å‡∏£‡πâ‡∏≤‡∏ô (store-level models) ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ó‡∏®‡∏Å‡∏≤‡∏•  \n",
    "\n",
    "---\n",
    "\n",
    "**English Summary:**  \n",
    "After tuning via Time-Series Cross-Validation,  \n",
    "the **XGBoost model** achieved a lower **MAPE** (6.40% ‚Üí 6.06%),  \n",
    "indicating improved relative accuracy, though **RMSE** increased slightly (97,804 ‚Üí 100,362).  \n",
    "\n",
    "Top contributing features remain business-consistent ‚Äî  \n",
    "`Store_avg_to_date`, `Weekly_Sales_Real_rollmean8`, and `CPI_lag1` ‚Äî  \n",
    "reflecting the importance of accumulated store performance and seasonality trends.  \n",
    "\n",
    "Bias analysis revealed higher MAPE during **holiday and peak months** (January, November),  \n",
    "suggesting that the model struggles with promotional or irregular demand patterns.  \n",
    "\n",
    "**Conclusion:**  \n",
    "- ‚úÖ Improved accuracy (lower MAPE)  \n",
    "- ‚ö†Ô∏è Slightly higher RMSE  \n",
    "- üéØ Bias observed during holiday/peak periods  \n",
    "\n",
    "**Next Steps:**  \n",
    "- Expand hyperparameter tuning (`min_child_weight`, `gamma`, `reg_lambda`)  \n",
    "- Add holiday-aware and seasonality-based features  \n",
    "- Increase cross-validation folds for robustness  \n",
    "- Explore store-level or seasonal segmentation models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
